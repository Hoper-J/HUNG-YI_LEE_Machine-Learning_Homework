{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Operations\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Reading/Writing Data\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# For Progress Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import optuna\n",
    "\n",
    "# For plotting learning curve\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovidDataset(Dataset):\n",
    "    def __init__(self, x, y=None):\n",
    "        if y is None:\n",
    "            self.y = y\n",
    "        else:\n",
    "            self.y = torch.FloatTensor(y)\n",
    "        self.x = torch.FloatTensor(x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.x[idx]\n",
    "        else:\n",
    "            return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(My_Model, self).__init__()\n",
    "        # TODO: modify model's structure, be aware of dimensions.\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, config['layer'][0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config['layer'][0], config['layer'][1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config['layer'][1], 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = x.squeeze(1) # (B, 1) -> (B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "def select_feat(train_data, valid_data, test_data, no_select_all=True):\n",
    "    '''Selects useful features to perform regression'''\n",
    "    global config\n",
    "    y_train, y_valid = train_data[:,-1], valid_data[:,-1]\n",
    "    raw_x_train, raw_x_valid, raw_x_test = train_data[:,:-1], valid_data[:,:-1], test_data\n",
    "\n",
    "    if not no_select_all:\n",
    "        feat_idx = list(range(raw_x_train.shape[1]))\n",
    "    else:\n",
    "        k = config['k']\n",
    "        selector = SelectKBest(score_func=f_regression, k=k)\n",
    "        result = selector.fit(train_data[:, :-1], train_data[:,-1])\n",
    "        idx = np.argsort(result.scores_)[::-1]\n",
    "        feat_idx = list(np.sort(idx[:k]))\n",
    "\n",
    "    return raw_x_train[:,feat_idx], raw_x_valid[:,feat_idx], raw_x_test[:,feat_idx], y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(train_loader, valid_loader, model, config, device):\n",
    "\n",
    "    criterion = nn.MSELoss(reduction='mean') # Define your loss function, do not modify this.\n",
    "    \n",
    "    # Define your optimization algorithm.\n",
    "    if config['optim'] == 'SGD':\n",
    "        if config['no_momentum']:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])     \n",
    "        else:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'], weight_decay=config['weight_decay'])     \n",
    "    elif config['optim'] == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "        \n",
    "    writer = SummaryWriter() # Writer of tensoboard.\n",
    "\n",
    "        \n",
    "    if not os.path.isdir('./models'):\n",
    "        os.mkdir('./models') # Create directory of saving models.\n",
    "\n",
    "    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train() # Set your model to train mode.\n",
    "        loss_record = []\n",
    "        \n",
    "        # 如果你在kaggle上运行，可以注释掉大部分的打印函数，并将train_pbar注释掉，令 x,y in train_loader，因为kaggle上打印太多可能会报错。\n",
    "        # tqdm is a package to visualize your training progress.\n",
    "#         train_pbar = tqdm(train_loader, position=0, leave=True)\n",
    "\n",
    "#         for x, y in train_pbar:\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()               # Set gradient to zero.\n",
    "            x, y = x.to(device), y.to(device)   # Move your data to device.\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()                     # Compute gradient(backpropagation).\n",
    "            optimizer.step()                    # Update parameters.\n",
    "            step += 1\n",
    "            loss_record.append(loss.detach().item())\n",
    "\n",
    "            # Display current epoch number and loss on tqdm progress bar.\n",
    "#             train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n",
    "#             train_pbar.set_postfix({'loss': loss.detach().item()})\n",
    "\n",
    "        mean_train_loss = sum(loss_record)/len(loss_record)\n",
    "\n",
    "        model.eval() # Set your model to evaluation mode.\n",
    "        loss_record = []\n",
    "        for x, y in valid_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(x)\n",
    "                loss = criterion(pred, y)\n",
    "\n",
    "            loss_record.append(loss.item())\n",
    "\n",
    "        mean_valid_loss = sum(loss_record)/len(loss_record)        \n",
    "        \n",
    "#         if epoch % 100 == 0:\n",
    "#             print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n",
    "\n",
    "        if not config['no_tensorboard']:\n",
    "            writer.add_scalar('Loss/train', mean_train_loss, step)\n",
    "            writer.add_scalar('Loss/valid', mean_valid_loss, step)\n",
    "\n",
    "        if mean_valid_loss < best_loss:\n",
    "            best_loss = mean_valid_loss\n",
    "            \n",
    "            # 一轮实验中保存 K 折交叉验证中单折表现最好的模型\n",
    "            if len(valid_scores):\n",
    "                if best_loss < min(valid_scores):\n",
    "                    torch.save(model.state_dict(), config['save_path']) # Save your best model\n",
    "#                     print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n",
    "                    print('Saving model with loss {:.3f}...'.format(best_loss))\n",
    "            else:\n",
    "                torch.save(model.state_dict(), config['save_path']) # Save your best model\n",
    "#                 print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n",
    "                print('Saving model with loss {:.3f}...'.format(best_loss))\n",
    "                \n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= config['early_stop']:\n",
    "            print('Best loss {:.3f}...'.format(best_loss))\n",
    "            print('\\nModel is not improving, so we halt the training session.')\n",
    "            break\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seed(seed):\n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def train_valid_split(data_set, valid_ratio, seed):\n",
    "    '''Split provided training data into training set and validation set'''\n",
    "    valid_set_size = int(valid_ratio * len(data_set))\n",
    "    train_set_size = len(data_set) - valid_set_size\n",
    "    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))\n",
    "    return np.array(train_set), np.array(valid_set)\n",
    "\n",
    "def predict(test_loader, model, device):\n",
    "    model.eval() # Set your model to evaluation mode.\n",
    "    preds = []\n",
    "    for x in tqdm(test_loader):\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(x)\n",
    "            preds.append(pred.detach().cpu())\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pred(preds, file):\n",
    "    ''' Save predictions to specified file '''\n",
    "    with open(file, 'w') as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow(['id', 'tested_positive'])\n",
    "        for i, p in enumerate(preds):\n",
    "            writer.writerow([i, p])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-13 20:11:46,430]\u001b[0m A new study created in memory with name: no-name-cd10ce44-fd26-4211-bcb3-e8d5673377ac\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUTO_TUNE_PARAM: True\n",
      "n_trials: 10\n",
      "\n",
      "New trial here\n",
      "hyper-parameter: \n",
      "        optimizer: SGD,\n",
      "        lr: 0.000811114680829132, \n",
      "        batch_size: 128, \n",
      "        k: 21, \n",
      "        layer: [21, 16]\n",
      "Saving model with loss 351.956...\n",
      "Saving model with loss 324.595...\n",
      "Saving model with loss 291.570...\n",
      "Saving model with loss 242.433...\n",
      "Saving model with loss 177.519...\n",
      "Saving model with loss 124.620...\n",
      "Saving model with loss 101.253...\n",
      "Saving model with loss 96.681...\n",
      "Saving model with loss 96.361...\n",
      "Saving model with loss 95.438...\n",
      "Saving model with loss 95.330...\n",
      "Saving model with loss 94.923...\n",
      "Saving model with loss 94.553...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-13 20:11:54,157]\u001b[0m Trial 0 finished with value: 94.55254364013672 and parameters: {'lr': 0.000811114680829132, 'batch_size': 128, 'k_feats': 21}. Best is trial 0 with value: 94.55254364013672.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss 94.553...\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "Cross validation:\n",
      "valid_scores: [94.55254364013672]\n",
      "\n",
      "New trial here\n",
      "hyper-parameter: \n",
      "        optimizer: SGD,\n",
      "        lr: 0.0006895145744494396, \n",
      "        batch_size: 128, \n",
      "        k: 21, \n",
      "        layer: [21, 16]\n",
      "Saving model with loss 359.117...\n",
      "Saving model with loss 344.474...\n",
      "Saving model with loss 322.227...\n",
      "Saving model with loss 297.654...\n",
      "Saving model with loss 267.903...\n",
      "Saving model with loss 225.116...\n",
      "Saving model with loss 175.905...\n",
      "Saving model with loss 130.755...\n",
      "Saving model with loss 106.668...\n",
      "Saving model with loss 97.867...\n",
      "Saving model with loss 97.806...\n",
      "Saving model with loss 94.899...\n",
      "Saving model with loss 94.776...\n",
      "Saving model with loss 94.599...\n",
      "Saving model with loss 93.481...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-13 20:12:07,218]\u001b[0m Trial 1 finished with value: 93.48096237182617 and parameters: {'lr': 0.0006895145744494396, 'batch_size': 128, 'k_feats': 21}. Best is trial 1 with value: 93.48096237182617.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss 93.481...\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "Cross validation:\n",
      "valid_scores: [93.48096237182617]\n",
      "\n",
      "New trial here\n",
      "hyper-parameter: \n",
      "        optimizer: SGD,\n",
      "        lr: 0.0009135076452715329, \n",
      "        batch_size: 128, \n",
      "        k: 18, \n",
      "        layer: [18, 16]\n",
      "Saving model with loss 371.981...\n",
      "Saving model with loss 348.382...\n",
      "Saving model with loss 323.051...\n",
      "Saving model with loss 283.563...\n",
      "Saving model with loss 220.953...\n",
      "Saving model with loss 150.395...\n",
      "Saving model with loss 107.475...\n",
      "Saving model with loss 96.676...\n",
      "Saving model with loss 95.612...\n",
      "Saving model with loss 95.322...\n",
      "Saving model with loss 95.239...\n",
      "Saving model with loss 95.203...\n",
      "Saving model with loss 94.703...\n",
      "Saving model with loss 94.247...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-13 20:12:15,789]\u001b[0m Trial 2 finished with value: 94.2474136352539 and parameters: {'lr': 0.0009135076452715329, 'batch_size': 128, 'k_feats': 18}. Best is trial 1 with value: 93.48096237182617.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss 94.247...\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "Cross validation:\n",
      "valid_scores: [94.2474136352539]\n",
      "\n",
      "New trial here\n",
      "hyper-parameter: \n",
      "        optimizer: SGD,\n",
      "        lr: 6.053244100739726e-05, \n",
      "        batch_size: 128, \n",
      "        k: 30, \n",
      "        layer: [30, 16]\n",
      "Saving model with loss 105.482...\n",
      "Saving model with loss 104.554...\n",
      "Saving model with loss 104.028...\n",
      "Saving model with loss 100.660...\n",
      "Saving model with loss 100.043...\n",
      "Saving model with loss 98.367...\n",
      "Saving model with loss 96.905...\n",
      "Saving model with loss 96.107...\n",
      "Saving model with loss 95.533...\n",
      "Saving model with loss 94.975...\n",
      "Saving model with loss 94.458...\n",
      "Saving model with loss 94.195...\n",
      "Saving model with loss 94.161...\n",
      "Saving model with loss 94.075...\n",
      "Saving model with loss 93.902...\n",
      "Saving model with loss 87.899...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-13 20:12:24,822]\u001b[0m Trial 3 finished with value: 87.8993133544922 and parameters: {'lr': 6.053244100739726e-05, 'batch_size': 128, 'k_feats': 30}. Best is trial 3 with value: 87.8993133544922.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss 87.899...\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "Cross validation:\n",
      "valid_scores: [87.8993133544922]\n",
      "\n",
      "New trial here\n",
      "hyper-parameter: \n",
      "        optimizer: SGD,\n",
      "        lr: 0.0008380496328811374, \n",
      "        batch_size: 128, \n",
      "        k: 18, \n",
      "        layer: [18, 16]\n",
      "Saving model with loss 367.381...\n",
      "Saving model with loss 348.489...\n",
      "Saving model with loss 324.569...\n",
      "Saving model with loss 299.970...\n",
      "Saving model with loss 256.700...\n",
      "Saving model with loss 207.332...\n",
      "Saving model with loss 146.768...\n",
      "Saving model with loss 110.705...\n",
      "Saving model with loss 99.097...\n",
      "Saving model with loss 96.173...\n",
      "Saving model with loss 95.926...\n",
      "Saving model with loss 95.512...\n",
      "Saving model with loss 95.262...\n",
      "Saving model with loss 95.116...\n",
      "Saving model with loss 94.805...\n",
      "Saving model with loss 94.778...\n",
      "Saving model with loss 94.563...\n",
      "Saving model with loss 94.454...\n",
      "Saving model with loss 94.388...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-13 20:12:38,924]\u001b[0m Trial 4 finished with value: 94.38846435546876 and parameters: {'lr': 0.0008380496328811374, 'batch_size': 128, 'k_feats': 18}. Best is trial 3 with value: 87.8993133544922.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss 94.388...\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "Cross validation:\n",
      "valid_scores: [94.38846435546876]\n",
      "\n",
      "New trial here\n",
      "hyper-parameter: \n",
      "        optimizer: SGD,\n",
      "        lr: 0.0008211123528225122, \n",
      "        batch_size: 128, \n",
      "        k: 19, \n",
      "        layer: [19, 16]\n",
      "Saving model with loss 364.101...\n",
      "Saving model with loss 346.440...\n",
      "Saving model with loss 329.909...\n",
      "Saving model with loss 311.618...\n",
      "Saving model with loss 296.637...\n",
      "Saving model with loss 275.760...\n",
      "Saving model with loss 244.259...\n",
      "Saving model with loss 203.048...\n",
      "Saving model with loss 162.494...\n",
      "Saving model with loss 119.846...\n",
      "Saving model with loss 102.065...\n",
      "Saving model with loss 95.720...\n",
      "Saving model with loss 94.685...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-13 20:12:45,814]\u001b[0m Trial 5 finished with value: 94.68473205566406 and parameters: {'lr': 0.0008211123528225122, 'batch_size': 128, 'k_feats': 19}. Best is trial 3 with value: 87.8993133544922.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss 94.685...\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "Cross validation:\n",
      "valid_scores: [94.68473205566406]\n",
      "\n",
      "New trial here\n",
      "hyper-parameter: \n",
      "        optimizer: SGD,\n",
      "        lr: 4.7393053784911005e-05, \n",
      "        batch_size: 128, \n",
      "        k: 28, \n",
      "        layer: [28, 16]\n",
      "Saving model with loss 53.594...\n",
      "Saving model with loss 29.480...\n",
      "Saving model with loss 15.976...\n",
      "Saving model with loss 10.228...\n",
      "Saving model with loss 9.429...\n",
      "Saving model with loss 8.979...\n",
      "Saving model with loss 8.682...\n",
      "Saving model with loss 8.360...\n",
      "Saving model with loss 8.069...\n",
      "Saving model with loss 7.917...\n",
      "Saving model with loss 7.174...\n",
      "Saving model with loss 6.975...\n",
      "Saving model with loss 6.666...\n",
      "Saving model with loss 6.324...\n",
      "Saving model with loss 6.121...\n",
      "Saving model with loss 5.859...\n",
      "Saving model with loss 5.596...\n",
      "Saving model with loss 5.419...\n",
      "Saving model with loss 5.301...\n",
      "Saving model with loss 4.721...\n",
      "Saving model with loss 4.543...\n",
      "Saving model with loss 4.117...\n",
      "Saving model with loss 3.930...\n",
      "Saving model with loss 3.635...\n",
      "Saving model with loss 3.446...\n",
      "Saving model with loss 3.209...\n",
      "Saving model with loss 2.951...\n",
      "Saving model with loss 2.779...\n",
      "Saving model with loss 2.759...\n",
      "Saving model with loss 2.524...\n",
      "Saving model with loss 2.430...\n",
      "Saving model with loss 2.316...\n",
      "Saving model with loss 2.177...\n",
      "Saving model with loss 2.138...\n",
      "Saving model with loss 1.979...\n",
      "Saving model with loss 1.853...\n",
      "Saving model with loss 1.797...\n",
      "Saving model with loss 1.692...\n",
      "Saving model with loss 1.589...\n",
      "Saving model with loss 1.565...\n",
      "Saving model with loss 1.515...\n",
      "Saving model with loss 1.451...\n",
      "Saving model with loss 1.363...\n",
      "Saving model with loss 1.348...\n",
      "Saving model with loss 1.326...\n",
      "Saving model with loss 1.277...\n",
      "Saving model with loss 1.255...\n",
      "Saving model with loss 1.240...\n",
      "Saving model with loss 1.235...\n",
      "Saving model with loss 1.197...\n",
      "Saving model with loss 1.183...\n",
      "Saving model with loss 1.159...\n",
      "Saving model with loss 1.139...\n",
      "Saving model with loss 1.135...\n",
      "Saving model with loss 1.135...\n",
      "Saving model with loss 1.126...\n",
      "Saving model with loss 1.113...\n",
      "Saving model with loss 1.093...\n",
      "Saving model with loss 1.085...\n",
      "Saving model with loss 1.077...\n",
      "Saving model with loss 1.074...\n",
      "Saving model with loss 1.068...\n",
      "Saving model with loss 1.064...\n",
      "Saving model with loss 1.062...\n",
      "Saving model with loss 1.052...\n",
      "Saving model with loss 1.046...\n",
      "Saving model with loss 1.043...\n",
      "Saving model with loss 1.039...\n",
      "Saving model with loss 1.036...\n",
      "Saving model with loss 1.016...\n",
      "Saving model with loss 1.009...\n",
      "Saving model with loss 0.985...\n",
      "Saving model with loss 0.984...\n",
      "Saving model with loss 0.977...\n",
      "Saving model with loss 0.976...\n",
      "Saving model with loss 0.970...\n",
      "Saving model with loss 0.966...\n",
      "Saving model with loss 0.955...\n",
      "Saving model with loss 0.950...\n",
      "Saving model with loss 0.942...\n",
      "Saving model with loss 0.935...\n",
      "Saving model with loss 0.933...\n",
      "Saving model with loss 0.932...\n",
      "Saving model with loss 0.930...\n",
      "Saving model with loss 0.927...\n",
      "Saving model with loss 0.919...\n",
      "Saving model with loss 0.918...\n",
      "Saving model with loss 0.911...\n",
      "Saving model with loss 0.898...\n",
      "Saving model with loss 0.895...\n",
      "Saving model with loss 0.890...\n",
      "Saving model with loss 0.889...\n",
      "Saving model with loss 0.878...\n",
      "Saving model with loss 0.877...\n",
      "Saving model with loss 0.873...\n",
      "Saving model with loss 0.864...\n",
      "Saving model with loss 0.863...\n",
      "Saving model with loss 0.863...\n",
      "Saving model with loss 0.862...\n",
      "Saving model with loss 0.855...\n",
      "Saving model with loss 0.852...\n",
      "Saving model with loss 0.842...\n",
      "Saving model with loss 0.841...\n",
      "Saving model with loss 0.841...\n",
      "Saving model with loss 0.840...\n",
      "Saving model with loss 0.838...\n",
      "Saving model with loss 0.829...\n",
      "Saving model with loss 0.828...\n",
      "Saving model with loss 0.826...\n",
      "Saving model with loss 0.825...\n",
      "Saving model with loss 0.819...\n",
      "Saving model with loss 0.817...\n",
      "Saving model with loss 0.812...\n",
      "Saving model with loss 0.809...\n",
      "Saving model with loss 0.807...\n",
      "Saving model with loss 0.807...\n",
      "Saving model with loss 0.797...\n",
      "Saving model with loss 0.797...\n",
      "Saving model with loss 0.790...\n",
      "Saving model with loss 0.787...\n",
      "Saving model with loss 0.786...\n",
      "Saving model with loss 0.786...\n",
      "Saving model with loss 0.784...\n",
      "Saving model with loss 0.777...\n",
      "Saving model with loss 0.777...\n",
      "Saving model with loss 0.776...\n",
      "Saving model with loss 0.775...\n",
      "Saving model with loss 0.773...\n",
      "Saving model with loss 0.771...\n",
      "Saving model with loss 0.767...\n",
      "Saving model with loss 0.766...\n",
      "Saving model with loss 0.766...\n",
      "Saving model with loss 0.766...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2023-03-13 20:13:42,224]\u001b[0m Trial 6 finished with value: 0.7657820105552673 and parameters: {'lr': 4.7393053784911005e-05, 'batch_size': 128, 'k_feats': 28}. Best is trial 6 with value: 0.7657820105552673.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best loss 0.766...\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "Cross validation:\n",
      "valid_scores: [0.7657820105552673]\n",
      "\n",
      "New trial here\n",
      "hyper-parameter: \n",
      "        optimizer: SGD,\n",
      "        lr: 1.3612682004472124e-05, \n",
      "        batch_size: 128, \n",
      "        k: 18, \n",
      "        layer: [18, 16]\n",
      "Saving model with loss 318.496...\n",
      "Saving model with loss 156.631...\n",
      "Saving model with loss 52.916...\n",
      "Saving model with loss 37.463...\n",
      "Saving model with loss 32.974...\n",
      "Saving model with loss 28.658...\n",
      "Saving model with loss 25.995...\n",
      "Saving model with loss 23.751...\n",
      "Saving model with loss 21.412...\n",
      "Saving model with loss 19.680...\n",
      "Saving model with loss 17.772...\n",
      "Saving model with loss 15.931...\n",
      "Saving model with loss 14.348...\n",
      "Saving model with loss 12.960...\n",
      "Saving model with loss 11.535...\n",
      "Saving model with loss 10.501...\n",
      "Saving model with loss 9.580...\n",
      "Saving model with loss 9.021...\n",
      "Saving model with loss 8.353...\n",
      "Saving model with loss 8.091...\n",
      "Saving model with loss 7.873...\n",
      "Saving model with loss 7.461...\n",
      "Saving model with loss 7.183...\n",
      "Saving model with loss 6.948...\n",
      "Saving model with loss 6.782...\n",
      "Saving model with loss 6.623...\n",
      "Saving model with loss 6.469...\n",
      "Saving model with loss 6.412...\n",
      "Saving model with loss 6.297...\n",
      "Saving model with loss 6.190...\n",
      "Saving model with loss 6.012...\n",
      "Saving model with loss 5.983...\n",
      "Saving model with loss 5.871...\n",
      "Saving model with loss 5.818...\n",
      "Saving model with loss 5.731...\n",
      "Saving model with loss 5.649...\n",
      "Saving model with loss 5.525...\n",
      "Saving model with loss 5.403...\n",
      "Saving model with loss 5.344...\n",
      "Saving model with loss 5.279...\n",
      "Saving model with loss 5.235...\n",
      "Saving model with loss 5.119...\n",
      "Saving model with loss 4.993...\n",
      "Saving model with loss 4.932...\n",
      "Saving model with loss 4.855...\n",
      "Saving model with loss 4.852...\n",
      "Saving model with loss 4.804...\n",
      "Saving model with loss 4.786...\n",
      "Saving model with loss 4.617...\n",
      "Saving model with loss 4.509...\n",
      "Saving model with loss 4.422...\n",
      "Saving model with loss 4.413...\n",
      "Saving model with loss 4.385...\n",
      "Saving model with loss 4.299...\n",
      "Saving model with loss 4.280...\n",
      "Saving model with loss 4.210...\n",
      "Saving model with loss 4.154...\n",
      "Saving model with loss 4.062...\n",
      "Saving model with loss 4.051...\n",
      "Saving model with loss 4.023...\n",
      "Saving model with loss 3.953...\n",
      "Saving model with loss 3.859...\n",
      "Saving model with loss 3.806...\n",
      "Saving model with loss 3.746...\n",
      "Saving model with loss 3.743...\n",
      "Saving model with loss 3.700...\n",
      "Saving model with loss 3.596...\n",
      "Saving model with loss 3.554...\n",
      "Saving model with loss 3.486...\n",
      "Saving model with loss 3.369...\n",
      "Saving model with loss 3.333...\n",
      "Saving model with loss 3.285...\n",
      "Saving model with loss 3.256...\n",
      "Saving model with loss 3.211...\n",
      "Saving model with loss 3.196...\n",
      "Saving model with loss 3.112...\n",
      "Saving model with loss 3.081...\n",
      "Saving model with loss 3.012...\n",
      "Saving model with loss 2.962...\n",
      "Saving model with loss 2.904...\n",
      "Saving model with loss 2.854...\n",
      "Saving model with loss 2.824...\n",
      "Saving model with loss 2.799...\n",
      "Saving model with loss 2.772...\n",
      "Saving model with loss 2.759...\n",
      "Saving model with loss 2.658...\n",
      "Saving model with loss 2.617...\n",
      "Saving model with loss 2.609...\n",
      "Saving model with loss 2.550...\n",
      "Saving model with loss 2.520...\n",
      "Saving model with loss 2.498...\n",
      "Saving model with loss 2.471...\n",
      "Saving model with loss 2.422...\n",
      "Saving model with loss 2.372...\n",
      "Saving model with loss 2.344...\n",
      "Saving model with loss 2.300...\n",
      "Saving model with loss 2.292...\n",
      "Saving model with loss 2.216...\n",
      "Saving model with loss 2.180...\n",
      "Saving model with loss 2.119...\n",
      "Saving model with loss 2.111...\n",
      "Saving model with loss 2.089...\n",
      "Saving model with loss 2.052...\n",
      "Saving model with loss 2.027...\n",
      "Saving model with loss 2.017...\n",
      "Saving model with loss 1.991...\n",
      "Saving model with loss 1.963...\n",
      "Saving model with loss 1.920...\n",
      "Saving model with loss 1.881...\n",
      "Saving model with loss 1.872...\n",
      "Saving model with loss 1.823...\n",
      "Saving model with loss 1.808...\n",
      "Saving model with loss 1.786...\n",
      "Saving model with loss 1.751...\n",
      "Saving model with loss 1.738...\n",
      "Saving model with loss 1.715...\n",
      "Saving model with loss 1.678...\n",
      "Saving model with loss 1.656...\n",
      "Saving model with loss 1.633...\n",
      "Saving model with loss 1.630...\n",
      "Saving model with loss 1.621...\n",
      "Saving model with loss 1.594...\n",
      "Saving model with loss 1.578...\n",
      "Saving model with loss 1.552...\n",
      "Saving model with loss 1.542...\n",
      "Saving model with loss 1.482...\n",
      "Saving model with loss 1.482...\n",
      "Saving model with loss 1.454...\n",
      "Saving model with loss 1.446...\n",
      "Saving model with loss 1.439...\n",
      "Saving model with loss 1.415...\n",
      "Saving model with loss 1.402...\n",
      "Saving model with loss 1.391...\n",
      "Saving model with loss 1.379...\n",
      "Saving model with loss 1.363...\n",
      "Saving model with loss 1.336...\n",
      "Saving model with loss 1.334...\n",
      "Saving model with loss 1.315...\n",
      "Saving model with loss 1.305...\n",
      "Saving model with loss 1.292...\n",
      "Saving model with loss 1.292...\n",
      "Saving model with loss 1.288...\n",
      "Saving model with loss 1.269...\n",
      "Saving model with loss 1.257...\n",
      "Saving model with loss 1.244...\n",
      "Saving model with loss 1.230...\n",
      "Saving model with loss 1.229...\n",
      "Saving model with loss 1.223...\n",
      "Saving model with loss 1.208...\n",
      "Saving model with loss 1.204...\n",
      "Saving model with loss 1.195...\n",
      "Saving model with loss 1.188...\n",
      "Saving model with loss 1.167...\n",
      "Saving model with loss 1.163...\n",
      "Saving model with loss 1.161...\n",
      "Saving model with loss 1.159...\n",
      "Saving model with loss 1.152...\n",
      "Saving model with loss 1.151...\n",
      "Saving model with loss 1.150...\n",
      "Saving model with loss 1.150...\n",
      "Saving model with loss 1.146...\n",
      "Saving model with loss 1.140...\n",
      "Saving model with loss 1.134...\n",
      "Saving model with loss 1.132...\n",
      "Saving model with loss 1.131...\n",
      "Saving model with loss 1.128...\n",
      "Saving model with loss 1.118...\n",
      "Saving model with loss 1.113...\n",
      "Saving model with loss 1.111...\n",
      "Saving model with loss 1.109...\n",
      "Saving model with loss 1.102...\n",
      "Saving model with loss 1.102...\n",
      "Saving model with loss 1.101...\n",
      "Saving model with loss 1.100...\n",
      "Saving model with loss 1.099...\n",
      "Saving model with loss 1.095...\n",
      "Saving model with loss 1.087...\n",
      "Saving model with loss 1.085...\n",
      "Saving model with loss 1.079...\n",
      "Saving model with loss 1.077...\n",
      "Saving model with loss 1.070...\n",
      "Saving model with loss 1.068...\n",
      "Saving model with loss 1.064...\n",
      "Saving model with loss 1.061...\n",
      "Saving model with loss 1.057...\n",
      "Saving model with loss 1.056...\n",
      "Saving model with loss 1.055...\n",
      "Saving model with loss 1.054...\n",
      "Saving model with loss 1.041...\n",
      "Saving model with loss 1.040...\n",
      "Saving model with loss 1.039...\n",
      "Saving model with loss 1.032...\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "config = {\n",
    "    'seed': 5201314,      # Your seed number, you can pick your lucky number. :)\n",
    "    'k': 16,    # Select k feature\n",
    "    'layer': [16, 16],\n",
    "    'optim': 'SGD',\n",
    "    'momentum': 0.7,\n",
    "    'valid_ratio': 0.2,   # validation_size = train_size * valid_ratio\n",
    "    'n_epochs': 10000,     # Number of epochs.\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 1e-5,\n",
    "    'weight_decay': 1e-5,\n",
    "    'early_stop': 600,    # If model has not improved for this many consecutive epochs, stop training.\n",
    "    'save_path': './models/model.ckpt',  # Your model will be saved here.\n",
    "    'no_select_all': True,   # Whether to use all features.\n",
    "    'no_momentum': True,   # Whether to use momentum\n",
    "    'no_normal': True,  # Whether to normalize data\n",
    "    'no_k_cross': False,     # Whether to use K-fold cross validation\n",
    "    'no_save': False,   # Whether to save model parameters\n",
    "    'no_tensorboard': False,  # Whether to write tensorboard\n",
    "} \n",
    "\n",
    "# 设置 k-fold 中的 k，这里是根据 valid_ratio 设定的\n",
    "k = int(1 / config['valid_ratio'])\n",
    "\n",
    " # Set seed for reproducibility\n",
    "same_seed(config['seed'])\n",
    "\n",
    "training_data, test_data = pd.read_csv('./covid_train.csv').values, pd.read_csv('./covid_test.csv').values\n",
    "    \n",
    "num_valid_samples = len(training_data) // k\n",
    "np.random.shuffle(training_data)\n",
    "valid_scores = []  # 记录 valid_loss\n",
    "\n",
    "def objective(trial):\n",
    "    if trial != None:\n",
    "        print('\\nNew trial here')\n",
    "        # 定义需要调优的超参数空间\n",
    "        config['learning_rate'] = trial.suggest_float('lr', 1e-6, 1e-3)\n",
    "        config['batch_size'] = trial.suggest_categorical('batch_size', [128])\n",
    "        config['k'] = trial.suggest_int('k_feats', 16, 32)\n",
    "        config['layer'][0] = config['k']\n",
    "    \n",
    "    # 打印所需的超参数\n",
    "    print(f'''hyper-parameter: \n",
    "        optimizer: {config['optim']},\n",
    "        lr: {config['learning_rate']}, \n",
    "        batch_size: {config['batch_size']}, \n",
    "        k: {config['k']}, \n",
    "        layer: {config['layer']}''')\n",
    "    \n",
    "    global valid_scores\n",
    "    # 每次 trial 初始化 valid_scores，可以不初始化，通过 trial * k + fold 来访问当前 trial 的 valid_score，\n",
    "    # 这样可以让 trainer() 保存 trials 中最好的模型参数，但这并不意味着该参数对应的 k-fold validation loss 最低。\n",
    "    valid_scores = []\n",
    "\n",
    "    for fold in range(k):\n",
    "        # Data split\n",
    "        valid_data = training_data[num_valid_samples * fold:\n",
    "                                num_valid_samples * (fold + 1)]\n",
    "        train_data = np.concatenate((\n",
    "            training_data[:num_valid_samples * fold],\n",
    "            training_data[num_valid_samples * (fold + 1):]))\n",
    "\n",
    "        # Normalization\n",
    "        if not config['no_normal']:\n",
    "            train_mean = np.mean(train_data[:, 35:-1], axis=0)  # 前 35 列为 one-hot vector，我并没有对他们做 normalization，可以自行设置\n",
    "            train_std = np.std(train_data[:, 35:-1], axis=0)\n",
    "            train_data[:, 35:-1] -= train_mean\n",
    "            train_data[:, 35:-1] /= train_std\n",
    "            valid_data[:, 35:-1] -= train_mean\n",
    "            valid_data[:, 35:-1] /= train_std\n",
    "            test_data[:, 35:] -= train_mean\n",
    "            test_data[:, 35:] /= train_std\n",
    "\n",
    "        x_train, x_valid, x_test, y_train, y_valid = select_feat(train_data, valid_data, test_data, config['no_select_all'])\n",
    "        \n",
    "        train_dataset, valid_dataset, test_dataset = CovidDataset(x_train, y_train), \\\n",
    "                                                CovidDataset(x_valid, y_valid), \\\n",
    "                                                CovidDataset(x_test)\n",
    "\n",
    "        # Pytorch data loader loads pytorch dataset into batches.\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True)\n",
    "        \n",
    "        model = My_Model(input_dim=x_train.shape[1]).to(device) # put your model and data on the same computation device.\n",
    "        valid_score = trainer(train_loader, valid_loader, model, config, device)\n",
    "        valid_scores.append(valid_score)\n",
    "        \n",
    "        if not config['no_k_cross']:\n",
    "            break\n",
    "            \n",
    "        if valid_score > 2:\n",
    "            print(f'在第{fold+1}折上欠拟合') # 提前终止，减少计算资源\n",
    "            break       \n",
    "        \n",
    "    print(f'valid_scores: {valid_scores}')\n",
    "    \n",
    "    if trial != None:\n",
    "        return np.average(valid_scores)\n",
    "    else:\n",
    "        return x_test, test_loader\n",
    "\n",
    "\n",
    "AUTO_TUNE_PARAM = True  # Whether to tune parameters automatically\n",
    "\n",
    "if AUTO_TUNE_PARAM:\n",
    "    # 使用Optuna库进行超参数搜索\n",
    "    n_trials = 10  # 设置试验数量\n",
    "    print(f'AUTO_TUNE_PARAM: {AUTO_TUNE_PARAM}\\nn_trials: {n_trials}')\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # 输出最优的超参数组合和性能指标\n",
    "    print('Best hyperparameters: {}'.format(study.best_params))\n",
    "    print('Best performance: {:.4f}'.format(study.best_value))\n",
    "else:\n",
    "    # 注意，只有非自动调参时才进行了predict，节省一下计算资源\n",
    "    print(f'You could set AUTO_TUNE_PARAM True to tune parameters automatically.\\nAUTO_TUNE_PARAM: {AUTO_TUNE_PARAM}')\n",
    "    x_test, test_loader = objective(None)\n",
    "    model = My_Model(input_dim=x_test.shape[1]).to(device)\n",
    "    model.load_state_dict(torch.load(config['save_path']))\n",
    "    preds = predict(test_loader, model, device)\n",
    "    save_pred(preds, 'submission.csv')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 136878), started 8:41:17 ago. (Use '!kill 136878' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-a4615226b4a8deac\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-a4615226b4a8deac\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=./runs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
