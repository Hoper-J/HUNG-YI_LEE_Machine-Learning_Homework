{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Operations\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Reading/Writing Data\n",
    "import pandas as pd\n",
    "import os\n",
    "import csv\n",
    "\n",
    "# For Progress Bar\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "# Matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optuna\n",
    "import optuna\n",
    "\n",
    "# For plotting learning curve\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some Utility Functions\n",
    "\n",
    "You do not need to modify this part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_seed(seed):\n",
    "    '''Fixes random number generator seeds for reproducibility.'''\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def train_valid_split(data_set, valid_ratio, seed):\n",
    "    '''Split provided training data into training set and validation set'''\n",
    "    valid_set_size = int(valid_ratio * len(data_set))\n",
    "    train_set_size = len(data_set) - valid_set_size\n",
    "    train_set, valid_set = random_split(data_set, [train_set_size, valid_set_size], generator=torch.Generator().manual_seed(seed))\n",
    "    return np.array(train_set), np.array(valid_set)\n",
    "\n",
    "def predict(test_loader, model, device):\n",
    "    model.eval() # Set your model to evaluation mode.\n",
    "    preds = []\n",
    "    for x in tqdm(test_loader):\n",
    "        x = x.to(device)\n",
    "        with torch.no_grad():\n",
    "            pred = model(x)\n",
    "            preds.append(pred.detach().cpu())\n",
    "    preds = torch.cat(preds, dim=0).numpy()\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CovidDataset(Dataset):\n",
    "    def __init__(self, x, y=None):\n",
    "        if y is None:\n",
    "            self.y = y\n",
    "        else:\n",
    "            self.y = torch.FloatTensor(y)\n",
    "        self.x = torch.FloatTensor(x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.x[idx]\n",
    "        else:\n",
    "            return self.x[idx], self.y[idx]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Model\n",
    "\n",
    "Try out different model architectures by modifying the class below. (You could tune config['layer'] to try)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Model(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(My_Model, self).__init__()\n",
    "        # TODO: modify model's structure, be aware of dimensions.\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(input_dim, config['layer'][0]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config['layer'][0], config['layer'][1]),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(config['layer'][1], 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layers(x)\n",
    "        x = x.squeeze(1) # (B, 1) -> (B)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "\n",
    "Choose features you deem useful by modifying the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_regression\n",
    "\n",
    "def select_feat(train_data, valid_data, test_data, no_select_all=True):\n",
    "    '''Selects useful features to perform regression'''\n",
    "    global config\n",
    "    y_train, y_valid = train_data[:,-1], valid_data[:,-1]\n",
    "    raw_x_train, raw_x_valid, raw_x_test = train_data[:,:-1], valid_data[:,:-1], test_data\n",
    "\n",
    "    if not no_select_all:\n",
    "        feat_idx = list(range(raw_x_train.shape[1]))\n",
    "    else:\n",
    "        # Feature selection\n",
    "        k = config['k']\n",
    "        selector = SelectKBest(score_func=f_regression, k=k)\n",
    "        result = selector.fit(train_data[:, :-1], train_data[:,-1])\n",
    "        idx = np.argsort(result.scores_)[::-1]\n",
    "        feat_idx = list(np.sort(idx[:k]))\n",
    "\n",
    "    return raw_x_train[:,feat_idx], raw_x_valid[:,feat_idx], raw_x_test[:,feat_idx], y_train, y_valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(train_loader, valid_loader, model, config, device):\n",
    "    \n",
    "    # Define your loss function, do not modify this.\n",
    "    criterion = nn.MSELoss(reduction='mean') \n",
    "    \n",
    "    # Define your optimization algorithm.\n",
    "    if config['optim'] == 'SGD':\n",
    "        if config['no_momentum']:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])     \n",
    "        else:\n",
    "            optimizer = torch.optim.SGD(model.parameters(), lr=config['learning_rate'], momentum=config['momentum'], weight_decay=config['weight_decay'])     \n",
    "    elif config['optim'] == 'Adam':\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=config['learning_rate'], weight_decay=config['weight_decay'])\n",
    "        \n",
    "    # Writer of tensoboard.\n",
    "    writer = SummaryWriter() \n",
    "\n",
    "        \n",
    "    if not os.path.isdir('./models'):\n",
    "        os.mkdir('./models') # Create directory of saving models.\n",
    "\n",
    "    n_epochs, best_loss, step, early_stop_count = config['n_epochs'], math.inf, 0, 0\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        model.train() # Set your model to train mode.\n",
    "        loss_record = []\n",
    "        \n",
    "        # 如果你在kaggle上运行，可以注释掉大部分的打印函数，并将train_pbar注释掉，令 x,y in train_loader，因为kaggle上打印太多可能会报错。\n",
    "        # tqdm is a package to visualize your training progress.\n",
    "        #train_pbar = tqdm(train_loader, position=0, leave=True)\n",
    "        #for x, y in train_pbar:\n",
    "        for x, y in train_loader:\n",
    "            optimizer.zero_grad()               # Set gradient to zero.\n",
    "            x, y = x.to(device), y.to(device)   # Move your data to device.\n",
    "            pred = model(x)\n",
    "            loss = criterion(pred, y)\n",
    "            loss.backward()                     # Compute gradient(backpropagation).\n",
    "            optimizer.step()                    # Update parameters.\n",
    "            step += 1\n",
    "            loss_record.append(loss.detach().item())\n",
    "\n",
    "            # Display current epoch number and loss on tqdm progress bar.\n",
    "            #train_pbar.set_description(f'Epoch [{epoch+1}/{n_epochs}]')\n",
    "            #train_pbar.set_postfix({'loss': loss.detach().item()})\n",
    "\n",
    "        mean_train_loss = sum(loss_record)/len(loss_record)\n",
    "\n",
    "        model.eval() # Set your model to evaluation mode.\n",
    "        loss_record = []\n",
    "        for x, y in valid_loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            with torch.no_grad():\n",
    "                pred = model(x)\n",
    "                loss = criterion(pred, y)\n",
    "\n",
    "            loss_record.append(loss.item())\n",
    "\n",
    "        mean_valid_loss = sum(loss_record)/len(loss_record)        \n",
    "        \n",
    "        #if epoch % 100 == 0:\n",
    "        #    print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n",
    "\n",
    "        if not config['no_tensorboard']:\n",
    "            writer.add_scalar('Loss/train', mean_train_loss, step)\n",
    "            writer.add_scalar('Loss/valid', mean_valid_loss, step)\n",
    "\n",
    "        if mean_valid_loss < best_loss:\n",
    "            best_loss = mean_valid_loss\n",
    "            \n",
    "            # 一轮实验中保存 K 折交叉验证中单折表现最好的模型\n",
    "            if len(valid_scores):\n",
    "                if best_loss < min(valid_scores):\n",
    "                    torch.save(model.state_dict(), config['save_path']) # Save your best model\n",
    "                    #print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n",
    "                    print('Saving model with loss {:.3f}...'.format(best_loss))\n",
    "            else:\n",
    "                torch.save(model.state_dict(), config['save_path']) # Save your best model\n",
    "                #print(f'Epoch [{epoch+1}/{n_epochs}]: Train loss: {mean_train_loss:.4f}, Valid loss: {mean_valid_loss:.4f}')\n",
    "                print('Saving model with loss {:.3f}...'.format(best_loss))\n",
    "                \n",
    "            early_stop_count = 0\n",
    "        else:\n",
    "            early_stop_count += 1\n",
    "\n",
    "        if early_stop_count >= config['early_stop']:\n",
    "            print('Best loss {:.3f}...'.format(best_loss))\n",
    "            print('\\nModel is not improving, so we halt the training session.')\n",
    "            break\n",
    "    return best_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_pred(preds, file):\n",
    "    ''' Save predictions to specified file '''\n",
    "    with open(file, 'w') as fp:\n",
    "        writer = csv.writer(fp)\n",
    "        writer.writerow(['id', 'tested_positive'])\n",
    "        for i, p in enumerate(preds):\n",
    "            writer.writerow([i, p])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start training!\n",
    "\n",
    "config contains hyper-parameters for training and the path to save your model.\n",
    "\n",
    "`objective()` is used for automatic parameter tuning, but you could set `AUTO_TUNE_PARAM` `False` to avoid it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You could set AUTO_TUNE_PARAM True to tune parameters automatically.\n",
      "AUTO_TUNE_PARAM: False\n",
      "hyper-parameter: \n",
      "        optimizer: SGD,\n",
      "        lr: 1e-05, \n",
      "        batch_size: 256, \n",
      "        k: 16, \n",
      "        layer: [16, 16]\n",
      "Saving model with loss 323.558...\n",
      "Saving model with loss 199.956...\n",
      "Saving model with loss 116.053...\n",
      "Saving model with loss 53.357...\n",
      "Saving model with loss 27.536...\n",
      "Saving model with loss 16.041...\n",
      "Saving model with loss 12.200...\n",
      "Saving model with loss 11.171...\n",
      "Saving model with loss 10.724...\n",
      "Saving model with loss 10.712...\n",
      "Saving model with loss 10.367...\n",
      "Saving model with loss 10.297...\n",
      "Saving model with loss 10.249...\n",
      "Saving model with loss 10.020...\n",
      "Saving model with loss 9.928...\n",
      "Saving model with loss 9.855...\n",
      "Saving model with loss 9.846...\n",
      "Saving model with loss 9.712...\n",
      "Saving model with loss 9.683...\n",
      "Saving model with loss 9.473...\n",
      "Saving model with loss 9.338...\n",
      "Saving model with loss 9.019...\n",
      "Saving model with loss 8.861...\n",
      "Saving model with loss 8.677...\n",
      "Saving model with loss 8.464...\n",
      "Saving model with loss 8.164...\n",
      "Saving model with loss 7.999...\n",
      "Saving model with loss 7.722...\n",
      "Saving model with loss 7.605...\n",
      "Saving model with loss 7.521...\n",
      "Saving model with loss 7.370...\n",
      "Saving model with loss 7.232...\n",
      "Saving model with loss 7.066...\n",
      "Saving model with loss 6.921...\n",
      "Saving model with loss 6.762...\n",
      "Saving model with loss 6.651...\n",
      "Saving model with loss 6.536...\n",
      "Saving model with loss 6.299...\n",
      "Saving model with loss 6.143...\n",
      "Saving model with loss 5.994...\n",
      "Saving model with loss 5.936...\n",
      "Saving model with loss 5.692...\n",
      "Saving model with loss 5.626...\n",
      "Saving model with loss 5.601...\n",
      "Saving model with loss 5.236...\n",
      "Saving model with loss 5.233...\n",
      "Saving model with loss 5.059...\n",
      "Saving model with loss 5.024...\n",
      "Saving model with loss 4.845...\n",
      "Saving model with loss 4.734...\n",
      "Saving model with loss 4.590...\n",
      "Saving model with loss 4.561...\n",
      "Saving model with loss 4.509...\n",
      "Saving model with loss 4.470...\n",
      "Saving model with loss 4.292...\n",
      "Saving model with loss 4.223...\n",
      "Saving model with loss 4.026...\n",
      "Saving model with loss 4.000...\n",
      "Saving model with loss 3.851...\n",
      "Saving model with loss 3.838...\n",
      "Saving model with loss 3.604...\n",
      "Saving model with loss 3.474...\n",
      "Saving model with loss 3.421...\n",
      "Saving model with loss 3.303...\n",
      "Saving model with loss 3.166...\n",
      "Saving model with loss 3.127...\n",
      "Saving model with loss 3.125...\n",
      "Saving model with loss 3.106...\n",
      "Saving model with loss 2.938...\n",
      "Saving model with loss 2.877...\n",
      "Saving model with loss 2.849...\n",
      "Saving model with loss 2.845...\n",
      "Saving model with loss 2.827...\n",
      "Saving model with loss 2.795...\n",
      "Saving model with loss 2.674...\n",
      "Saving model with loss 2.650...\n",
      "Saving model with loss 2.583...\n",
      "Saving model with loss 2.575...\n",
      "Saving model with loss 2.571...\n",
      "Saving model with loss 2.559...\n",
      "Saving model with loss 2.520...\n",
      "Saving model with loss 2.375...\n",
      "Saving model with loss 2.368...\n",
      "Saving model with loss 2.361...\n",
      "Saving model with loss 2.347...\n",
      "Saving model with loss 2.335...\n",
      "Saving model with loss 2.253...\n",
      "Saving model with loss 2.244...\n",
      "Saving model with loss 2.220...\n",
      "Saving model with loss 2.206...\n",
      "Saving model with loss 2.141...\n",
      "Saving model with loss 2.102...\n",
      "Saving model with loss 2.077...\n",
      "Saving model with loss 2.036...\n",
      "Saving model with loss 2.027...\n",
      "Saving model with loss 1.961...\n",
      "Saving model with loss 1.892...\n",
      "Saving model with loss 1.882...\n",
      "Saving model with loss 1.853...\n",
      "Saving model with loss 1.818...\n",
      "Saving model with loss 1.756...\n",
      "Saving model with loss 1.701...\n",
      "Saving model with loss 1.682...\n",
      "Saving model with loss 1.676...\n",
      "Saving model with loss 1.647...\n",
      "Saving model with loss 1.635...\n",
      "Saving model with loss 1.614...\n",
      "Saving model with loss 1.599...\n",
      "Saving model with loss 1.588...\n",
      "Saving model with loss 1.570...\n",
      "Saving model with loss 1.564...\n",
      "Saving model with loss 1.561...\n",
      "Saving model with loss 1.556...\n",
      "Saving model with loss 1.531...\n",
      "Saving model with loss 1.482...\n",
      "Saving model with loss 1.439...\n",
      "Saving model with loss 1.396...\n",
      "Saving model with loss 1.393...\n",
      "Saving model with loss 1.358...\n",
      "Saving model with loss 1.326...\n",
      "Saving model with loss 1.299...\n",
      "Saving model with loss 1.290...\n",
      "Saving model with loss 1.272...\n",
      "Saving model with loss 1.265...\n",
      "Saving model with loss 1.240...\n",
      "Saving model with loss 1.231...\n",
      "Saving model with loss 1.220...\n",
      "Saving model with loss 1.195...\n",
      "Saving model with loss 1.179...\n",
      "Saving model with loss 1.176...\n",
      "Saving model with loss 1.159...\n",
      "Saving model with loss 1.157...\n",
      "Saving model with loss 1.142...\n",
      "Saving model with loss 1.139...\n",
      "Saving model with loss 1.139...\n",
      "Saving model with loss 1.128...\n",
      "Saving model with loss 1.116...\n",
      "Saving model with loss 1.093...\n",
      "Saving model with loss 1.090...\n",
      "Saving model with loss 1.077...\n",
      "Saving model with loss 1.076...\n",
      "Saving model with loss 1.070...\n",
      "Saving model with loss 1.066...\n",
      "Saving model with loss 1.065...\n",
      "Saving model with loss 1.055...\n",
      "Saving model with loss 1.054...\n",
      "Saving model with loss 1.042...\n",
      "Saving model with loss 1.019...\n",
      "Saving model with loss 1.014...\n",
      "Saving model with loss 1.004...\n",
      "Saving model with loss 0.996...\n",
      "Saving model with loss 0.976...\n",
      "Saving model with loss 0.966...\n",
      "Saving model with loss 0.966...\n",
      "Saving model with loss 0.965...\n",
      "Saving model with loss 0.964...\n",
      "Saving model with loss 0.961...\n",
      "Saving model with loss 0.956...\n",
      "Saving model with loss 0.941...\n",
      "Saving model with loss 0.931...\n",
      "Saving model with loss 0.921...\n",
      "Saving model with loss 0.920...\n",
      "Saving model with loss 0.910...\n",
      "Saving model with loss 0.905...\n",
      "Saving model with loss 0.901...\n",
      "Saving model with loss 0.892...\n",
      "Saving model with loss 0.884...\n",
      "Saving model with loss 0.876...\n",
      "Saving model with loss 0.872...\n",
      "Saving model with loss 0.834...\n",
      "Best loss 0.834...\n",
      "\n",
      "Model is not improving, so we halt the training session.\n",
      "valid_scores: [0.8343698779741923]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 4/4 [00:00<00:00, 2854.24it/s]\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "config = {\n",
    "    'seed': 5201314,      # Your seed number, you can pick your lucky number. :)\n",
    "    'k': 16,              # Select k features\n",
    "    'layer': [16, 16],\n",
    "    'optim': 'SGD',\n",
    "    'momentum': 0.7,\n",
    "    'valid_ratio': 0.2,   # validation_size = train_size * valid_ratio\n",
    "    'n_epochs': 10000,    # Number of epochs.\n",
    "    'batch_size': 256,\n",
    "    'learning_rate': 1e-5,\n",
    "    'weight_decay': 1e-5,\n",
    "    'early_stop': 600,        # If model has not improved for this many consecutive epochs, stop training.\n",
    "    'save_path': './models/model.ckpt',  # Your model will be saved here.\n",
    "    'no_select_all': True,    # Whether to use all features.\n",
    "    'no_momentum': True,      # Whether to use momentum\n",
    "    'no_normal': True,        # Whether to normalize data\n",
    "    'no_k_cross': False,      # Whether to use K-fold cross validation\n",
    "    'no_save': False,         # Whether to save model parameters\n",
    "    'no_tensorboard': False,  # Whether to write tensorboard\n",
    "} \n",
    "\n",
    "# 设置 k-fold 中的 k，这里是根据 valid_ratio 设定的\n",
    "k = int(1 / config['valid_ratio'])\n",
    "\n",
    " # Set seed for reproducibility\n",
    "same_seed(config['seed'])\n",
    "\n",
    "training_data, test_data = pd.read_csv('./covid_train.csv').values, pd.read_csv('./covid_test.csv').values\n",
    "    \n",
    "num_valid_samples = len(training_data) // k\n",
    "np.random.shuffle(training_data)\n",
    "valid_scores = []  # 记录 valid_loss\n",
    "\n",
    "def objective(trial):\n",
    "    if trial != None:\n",
    "        print('\\nNew trial here')\n",
    "        # 定义需要调优的超参数空间\n",
    "        config['learning_rate'] = trial.suggest_float('lr', 1e-6, 1e-3)\n",
    "        config['batch_size'] = trial.suggest_categorical('batch_size', [128])\n",
    "        config['k'] = trial.suggest_int('k_feats', 16, 32)\n",
    "        config['layer'][0] = config['k']\n",
    "    \n",
    "    # 打印所需的超参数\n",
    "    print(f'''hyper-parameter: \n",
    "        optimizer: {config['optim']},\n",
    "        lr: {config['learning_rate']}, \n",
    "        batch_size: {config['batch_size']}, \n",
    "        k: {config['k']}, \n",
    "        layer: {config['layer']}''')\n",
    "    \n",
    "    global valid_scores\n",
    "    # 每次 trial 初始化 valid_scores，可以不初始化，通过 trial * k + fold 来访问当前 trial 的 valid_score，\n",
    "    # 这样可以让 trainer() 保存 trials 中最好的模型参数，但这并不意味着该参数对应的 k-fold validation loss 最低。\n",
    "    valid_scores = []\n",
    "\n",
    "    for fold in range(k):\n",
    "        # Data split\n",
    "        valid_data = training_data[num_valid_samples * fold:\n",
    "                                num_valid_samples * (fold + 1)]\n",
    "        train_data = np.concatenate((\n",
    "            training_data[:num_valid_samples * fold],\n",
    "            training_data[num_valid_samples * (fold + 1):]))\n",
    "\n",
    "        # Normalization\n",
    "        if not config['no_normal']:\n",
    "            train_mean = np.mean(train_data[:, 35:-1], axis=0)  # 前 35 列为 one-hot vector，我并没有对他们做 normalization，可以自行设置\n",
    "            train_std = np.std(train_data[:, 35:-1], axis=0)\n",
    "            train_data[:, 35:-1] -= train_mean\n",
    "            train_data[:, 35:-1] /= train_std\n",
    "            valid_data[:, 35:-1] -= train_mean\n",
    "            valid_data[:, 35:-1] /= train_std\n",
    "            test_data[:, 35:] -= train_mean\n",
    "            test_data[:, 35:] /= train_std\n",
    "\n",
    "        x_train, x_valid, x_test, y_train, y_valid = select_feat(train_data, valid_data, test_data, config['no_select_all'])\n",
    "        \n",
    "        train_dataset, valid_dataset, test_dataset = CovidDataset(x_train, y_train), \\\n",
    "                                                CovidDataset(x_valid, y_valid), \\\n",
    "                                                CovidDataset(x_test)\n",
    "\n",
    "        # Pytorch data loader loads pytorch dataset into batches.\n",
    "        train_loader = DataLoader(train_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
    "        valid_loader = DataLoader(valid_dataset, batch_size=config['batch_size'], shuffle=True, pin_memory=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=config['batch_size'], shuffle=False, pin_memory=True)\n",
    "        \n",
    "        model = My_Model(input_dim=x_train.shape[1]).to(device) # put your model and data on the same computation device.\n",
    "        valid_score = trainer(train_loader, valid_loader, model, config, device)\n",
    "        valid_scores.append(valid_score)\n",
    "        \n",
    "        if not config['no_k_cross']:\n",
    "            break\n",
    "            \n",
    "        if valid_score > 2:\n",
    "            print(f'在第{fold+1}折上欠拟合') # 提前终止，减少计算资源\n",
    "            break       \n",
    "        \n",
    "    print(f'valid_scores: {valid_scores}')\n",
    "    \n",
    "    if trial != None:\n",
    "        return np.average(valid_scores)\n",
    "    else:\n",
    "        return x_test, test_loader\n",
    "\n",
    "\n",
    "AUTO_TUNE_PARAM = False  # Whether to tune parameters automatically\n",
    "\n",
    "if AUTO_TUNE_PARAM:\n",
    "    # 使用Optuna库进行超参数搜索\n",
    "    n_trials = 10  # 设置试验数量\n",
    "    print(f'AUTO_TUNE_PARAM: {AUTO_TUNE_PARAM}\\nn_trials: {n_trials}')\n",
    "    study = optuna.create_study(direction='minimize')\n",
    "    study.optimize(objective, n_trials=n_trials)\n",
    "\n",
    "    # 输出最优的超参数组合和性能指标\n",
    "    print('Best hyperparameters: {}'.format(study.best_params))\n",
    "    print('Best performance: {:.4f}'.format(study.best_value))\n",
    "else:\n",
    "    # 注意，只有非自动调参时才进行了predict，节省一下计算资源\n",
    "    print(f'You could set AUTO_TUNE_PARAM True to tune parameters automatically.\\nAUTO_TUNE_PARAM: {AUTO_TUNE_PARAM}')\n",
    "    x_test, test_loader = objective(None)\n",
    "    model = My_Model(input_dim=x_test.shape[1]).to(device)\n",
    "    model.load_state_dict(torch.load(config['save_path']))\n",
    "    preds = predict(test_loader, model, device)\n",
    "    save_pred(preds, 'submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot learning curves with `tensorboard` (optional)\n",
    "\n",
    "`tensorboard` is a tool that allows you to visualize your training progress.\n",
    "\n",
    "If this block does not display your learning curve, please wait for few minutes, and re-run this block. It might take some time to load your logging information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9fab987f7fe73d17\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9fab987f7fe73d17\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%reload_ext tensorboard\n",
    "%tensorboard --logdir=./runs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
